


# PDF Knowledge Base QA System

Sistema de perguntas e respostas baseado em documentos PDF com processamento de linguagem natural usando LangChain e Ollama.

## ğŸ“‹ PrÃ©-requisitos

### InstalaÃ§Ã£o do Ollama
1. Instale o Ollama seguindo as instruÃ§Ãµes em: [Ollama Installation](https://ollama.ai/download)
2. Verifique se o Ollama estÃ¡ rodando:
```bash
ollama serve
```

### InstalaÃ§Ã£o dos Modelos LLM
Execute os seguintes comandos para baixar os modelos necessÃ¡rios:

```bash
# Modelo principal para QA
ollama pull mistral:7b-instruct

# Modelo para embeddings
ollama pull nomic-embed-text
```

### DependÃªncias Python
Instale as dependÃªncias do projeto:

```bash
pip install -r requirements.txt
```

## ğŸš€ ConfiguraÃ§Ã£o do Ambiente

1. Clone o repositÃ³rio:
```bash
git clone [URL_DO_SEU_REPOSITORIO]
cd pdf_knowledge_base
```

2. Crie um ambiente virtual Python:
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
.\venv\Scripts\activate  # Windows
```

3. Crie a estrutura de diretÃ³rios necessÃ¡ria:
```
pdf_knowledge_base/
â”œâ”€â”€ pdfs/            # Coloque seus PDFs aqui
â”œâ”€â”€ data/           # Dados processados
â”œâ”€â”€ exports/        # ExportaÃ§Ãµes do sistema
â”œâ”€â”€ extracted_texts/# Textos extraÃ­dos dos PDFs
â””â”€â”€ logs/           # Logs do sistema
```

## ğŸ’» Uso

1. Coloque seus arquivos PDF na pasta `PDF/`

2. Execute o processamento dos PDFs:
```bash
python src/pdf_processor.py
```

3. Inicie a interface Streamlit:
```bash
streamlit run src/streamlit_app.py
```

## ğŸ› ï¸ Funcionalidades

- Processamento de mÃºltiplos arquivos PDF
- ExtraÃ§Ã£o de texto com manutenÃ§Ã£o de formataÃ§Ã£o
- Sistema de QA baseado em contexto
- Interface web interativa
- Sistema de feedback
- ExportaÃ§Ã£o de histÃ³rico de conversas
- Logging detalhado

## ğŸ“Š Estrutura do Sistema

```
src/
â”œâ”€â”€ pdf_processor.py     # Processamento de PDFs
â”œâ”€â”€ qa_system.py         # Sistema de QA
â””â”€â”€ streamlit_app.py     # Interface web
```

## âš™ï¸ ConfiguraÃ§Ãµes AvanÃ§adas

### Ajuste de ParÃ¢metros

Os principais parÃ¢metros configurÃ¡veis estÃ£o em `qa_system.py`:

- `chunk_size`: Tamanho dos chunks de texto (default: 500)
- `chunk_overlap`: SobreposiÃ§Ã£o entre chunks (default: 100)
- `temperature`: Temperatura do modelo LLM (default: 0.3)
- `k`: NÃºmero de documentos relevantes a recuperar (default: 4)

### Modelos Alternativos

VocÃª pode usar outros modelos do Ollama:
```bash
# Listar modelos disponÃ­veis
ollama list

# Instalar modelos alternativos
ollama pull llama2
ollama pull codellama
```

## ğŸ“ Logging

O sistema mantÃ©m logs detalhados em `logs/`:
- InicializaÃ§Ã£o do sistema
- Processamento de documentos
- Queries e respostas
- Erros e exceÃ§Ãµes

## ğŸ¤ Contribuindo

1. Fork o projeto
2. Crie sua Feature Branch (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a Branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a [SUA_LICENÃ‡A] - veja o arquivo LICENSE.md para detalhes

## ğŸ¯ Status do Projeto

- [x] Processamento bÃ¡sico de PDF
- [x] Sistema de QA
- [x] Interface Streamlit
- [ ] Suporte a mÃºltiplos idiomas
- [ ] Melhorias na interface
- [ ] OtimizaÃ§Ã£o de performance

## ğŸš¨ Troubleshooting

### Problemas Comuns

1. **Erro de conexÃ£o com Ollama:**
   ```bash
   # Verifique se o serviÃ§o estÃ¡ rodando
   ollama serve
   ```

2. **MemÃ³ria insuficiente:**
   - Reduza o `chunk_size`
   - Processe menos documentos por vez

3. **Erros de GPU:**
   - Verifique os requisitos de CUDA
   - Use a versÃ£o CPU se necessÃ¡rio

## ğŸ“ Suporte

- Abra uma issue para reportar bugs
- DiscussÃµes para features


## ğŸ™ Agradecimentos

- [LangChain](https://github.com/hwchase17/langchain)
- [Ollama](https://ollama.ai)
- [Streamlit](https://streamlit.io)
```



